..  List of parameters is auto generated by LightGBM\helpers\parameter_generator.py from LightGBM\include\LightGBM\config.h file.

.. role:: raw-html(raw)
    :format: html

파라미터
==========

이 페이지는 LightGBM의 모든 파라미터에 대한 설명을 다룹니다.

**기타 유용한 링크 목록**

- `파이썬 API <./Python-API.rst>`__

- `파라미터 튜닝 <./Parameters-Tuning.rst>`__

**외부 링크**

- `Laurae++ 인터랙티브 문서`_

파라미터 형식
-----------------

파라미터를 지정하기 위해 ``key1=value1 key2=value2 ...`` 형식을 사용합니다.
설정 파일 또는 커맨드 라인으로 파라미터를 설정할 수 있습니다.
커맨드 라인을 사용하는 경우, ``=`` 의 앞뒤로 공백을 허용하지 않습니다.
설정 파일을 사용하는 경우, 하나의 라인에 하나의 파라미터만 허용합니다. ``#`` 을 활용해 주석을 달 수 있습니다.

만약 특정 파라미터가 커맨드 라인과 설정 파일에 모두 등장할 경우, LightGBM은 커맨드 라인의 파라미터를 우선합니다.

Python 및 R 패키지의 경우, 해당 언어의 기본 배열 타입(대개 ``multi-int`` 또는 ``multi-double`` 와 같은 ``multi-xxx`` 형태)을 파라미터로 받습니다.
예를 들어, ``monotone_constraints`` 파라미터를 다음과 같이 설정할 수 있습니다.

**Python**

.. code-block:: python

   params = {
      "monotone_constraints": [-1, 0, 1]
   }


**R**

.. code-block:: r

   params <- list(
      monotone_constraints = c(-1, 0, 1)
   )

.. start params list

주요 파라미터
---------------

-  ``config`` :raw-html:`<a id="config" title="Permalink to this parameter" href="#config">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열, 별칭: ``config_file``

   -  설정 파일의 경로

   -  **참고**: CLI 버전에서만 사용가능합니다.

-  ``task`` :raw-html:`<a id="task" title="Permalink to this parameter" href="#task">&#x1F517;&#xFE0E;</a>`, 기본값 = ``train``, 타입 = 열거형, 옵션: ``train``, ``predict``, ``convert_model``, ``refit``, 별칭: ``task_type``

   -  ``train`` 은 학습에 사용됩니다. 별칭: ``training``.

   -  ``predict`` 은 예측에 사용됩니다. 별칭: ``prediction``, ``test``.

   -  ``convert_model`` 은 모델을 if-else 형태로 변환합니다. 자세한 정보는 `변환 파라미터 <#convert-parameters>`__ 에서 확인하세요.

   -  ``refit`` 은 기존의 모델을 새로운 데이터에 재학습할 때 사용합니다. 별칭: ``refit_tree``.

   -  ``save_binary`` 은 학습 (및 검증) 데이터를 불러와 바이너리(binary) 형태로 저장합니다. 일반적인 사용법: ``save_binary`` 를 먼저 사용한 후, 저장된 바이너리 파일을 활용해 여러 개의 ``train`` 작업을 병렬로 실행합니다.

   -  **참고**: CLI 버전에서만 사용 가능합니다. 각 언어별 패키지가 제공하는 동일한 기능의 함수를 사용할 수도 있습니다.

-  ``objective`` :raw-html:`<a id="objective" title="Permalink to this parameter" href="#objective">&#x1F517;&#xFE0E;</a>`, 기본값 = ``regression``, 타입 = 열거형, 옵션: ``regression``, ``regression_l1``, ``huber``, ``fair``, ``poisson``, ``quantile``, ``mape``, ``gamma``, ``tweedie``, ``binary``, ``multiclass``, ``multiclassova``, ``cross_entropy``, ``cross_entropy_lambda``, ``lambdarank``, ``rank_xendcg``, 별칭: ``objective_type``, ``app``, ``application``, ``loss``

   -  회귀에서의 활용

      -  ``regression``, L2 loss, 별칭: ``regression_l2``, ``l2``, ``mean_squared_error``, ``mse``, ``l2_root``, ``root_mean_squared_error``, ``rmse``

      -  ``regression_l1``, L1 loss, 별칭: ``l1``, ``mean_absolute_error``, ``mae``

      -  ``huber``, `후버 손실(Huber Loss) <https://en.wikipedia.org/wiki/Huber_loss>`__

      -  ``fair``, `공정 손실(Fair Loss) <https://www.kaggle.com/c/allstate-claims-severity/discussion/24520>`__

      -  ``poisson``, `포아송 회귀(Poisson Regression) <https://en.wikipedia.org/wiki/Poisson_regression>`__

      -  ``quantile``, `분위수 회귀(Quantile Regression) <https://en.wikipedia.org/wiki/Quantile_regression>`__

      -  ``mape``, `평균 절대 비율 오차 손실(MAPE loss) <https://en.wikipedia.org/wiki/Mean_absolute_percentage_error>`__, 별칭: ``mean_absolute_percentage_error``

      -  ``gamma`` 는 로그 링크(Log-link)를 활용한 감마 회귀(Gamma Regression)에 사용됩니다. 보험 청구의 심도(Severity) 혹은 `감마 분포 <https://en.wikipedia.org/wiki/Gamma_distribution#Occurrence_and_applications>`__ 를 따르는 대상을 모델링할 때 유용합니다.

      -  ``tweedie`` 는 로그 링크(Log-link)를 활용한 트위디 회귀(Tweedie Regression)에 사용됩니다. 보험에서의 전손(Total Loss) 혹은 `트위디 분포 <https://en.wikipedia.org/wiki/Tweedie_distribution#Occurrence_and_applications>`__ 를 따르는 대상을 모델링할 때 유용합니다.

   -  이진(Binary) 분류에서의 활용

      -  ``binary`` 는 이진 `로그 손실(Log Loss) <https://en.wikipedia.org/wiki/Cross_entropy>`__ 분류 (또는 로지스틱 회귀(Logistic Regression))에 사용됩니다.

      -  0 또는 1의 라벨이 필요합니다. 0과 1 사이의 확률 라벨을 활용하는 ``cross-entropy`` 에서의 응용을 참고하세요.

   -  다중(Multi-class) 분류에서의 활용

      -  ``multiclass``, 목적 함수 `softmax <https://en.wikipedia.org/wiki/Softmax_function>`__ 을 활용합니다. 별칭: ``softmax``

      -  ``multiclassova``, `One-vs-All <https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest>`__ 이진(Binary) 목적 함수, 별칭: ``multiclass_ova``, ``ova``, ``ovr``

      -  ``num_class`` should be set as well

   -  교차 엔트로피(Cross Entropy)의 활용

      -  ``cross_entropy``, 교차 엔트로피에 대한 목적 함수 (선형 가중치 옵션 포함), 별칭: ``xentropy``

      -  ``cross_entropy_lambda``, 교차 엔트로피를 대체하는 재매개변수화(reparametrization), 별칭: ``xentlambda``

      -  라벨은 0과 1사이의 값을 가집니다.

   -  순위 예측에서의 응용

      -  ``lambdarank``, `lambdarank <https://papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf>`__ 목적 함수. `label_gain <#label_gain>`__ 을 사용하여 ``int`` 라벨의 gain(가중치)를 설정할 수 있으며, ``label`` 의 모든 값은 ``label_gain`` 의 개수 보다 적어야합니다.

      -  ``rank_xendcg``, `XE_NDCG_MART <https://arxiv.org/abs/1911.09798>`__ 랭킹 목적 함수, 별칭: ``xendcg``, ``xe_ndcg``, ``xe_ndcg_mart``, ``xendcg_mart``

      -  ``rank_xendcg`` 는 ``lambdarank`` 보다 빠르면서도 비슷한 수준의 성능을 보입니다.

      -  라벨은 ``int`` 타입만 허용합니다. 큰 숫자는 높은 순위를 의미합니다. (예: 0:나쁨, 1:보통, 2:좋음, 3:완벽)

-  ``boosting`` :raw-html:`<a id="boosting" title="Permalink to this parameter" href="#boosting">&#x1F517;&#xFE0E;</a>`, 기본값 = ``gbdt``, 타입 = 열거형, 옵션: ``gbdt``, ``rf``, ``dart``, ``goss``, 별칭: ``boosting_type``, ``boost``

   -  ``gbdt``, 기존의 그라디언트 부스팅 의사 결정 나무(Gradient Boosting Decision Tree), 별칭: ``gbrt``

   -  ``rf``, 랜덤 포레스트(Random Forest), 별칭: ``random_forest``

   -  ``dart``, `Dropouts meet Multiple Additive Regression Trees <https://arxiv.org/abs/1505.01866>`__

   -  ``goss``, 경사 기반 단측 샘플링(Gradient-based One-Side Sampling)

      -  **참고**: LightGBM은 첫 ``1 / learning_rate`` 회의 반복동안 ``gbdt`` 모드를 사용합니다.

-  ``data`` :raw-html:`<a id="data" title="Permalink to this parameter" href="#data">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열, 별칭: ``train``, ``train_data``, ``train_data_file``, ``data_filename``

   -  학습 데이터의 경로를 지정하면, LightGBM은 해당 경로의 데이터로 학습합니다.

   -  **참고**: CLI 버전에서만 사용 가능합니다.

-  ``valid`` :raw-html:`<a id="valid" title="Permalink to this parameter" href="#valid">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열, 별칭: ``test``, ``valid_data``, ``valid_data_file``, ``test_data``, ``test_data_file``, ``valid_filenames``

   -  검증/테스트 데이터의 경로를 지정하면 LightGBM이 평가 지표(Metric)에 기반하여 해당 데이터에 대한 결과를 출력합니다.

   -  ``,`` 를 활용하여 여러 개의 검증 데이터를 사용할 수 있습니다.

   -  **참고**: CLI 버전에서만 사용 가능합니다.

-  ``num_iterations`` :raw-html:`<a id="num_iterations" title="Permalink to this parameter" href="#num_iterations">&#x1F517;&#xFE0E;</a>`, 기본값 = ``100``, 타입 = 정수형, 별칭: ``num_iteration``, ``n_iter``, ``num_tree``, ``num_trees``, ``num_round``, ``num_rounds``, ``num_boost_round``, ``n_estimators``, ``max_iter``, 제약 조건: ``num_iterations >= 0``

   -  부스팅 반복 횟수

   -  **참고**: LightGBM은 다중 분류 문제에서 ``num_class * num_iterations`` 개의 트리를 만듭니다.

-  ``learning_rate`` :raw-html:`<a id="learning_rate" title="Permalink to this parameter" href="#learning_rate">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.1``, 타입 = 부동 소숫점(double), 별칭: ``shrinkage_rate``, ``eta``, 제약 조건: ``learning_rate > 0.0``

   -  수축률(Shrinkage Rate)

   -  ``dart`` 옵션을 설정할 경우,  ``learning_rate`` 는 제거된 트리(dropped trees)의 정규화 가중치(normalization weights)에도 영향을 미칩니다.

-  ``num_leaves`` :raw-html:`<a id="num_leaves" title="Permalink to this parameter" href="#num_leaves">&#x1F517;&#xFE0E;</a>`, 기본값 = ``31``, 타입 = 정수형, 별칭: ``num_leaf``, ``max_leaves``, ``max_leaf``, ``max_leaf_nodes``, 제약 조건: ``1 < num_leaves <= 131072``

   -  한 트리의 최대 리프 노드(leaf node)의 개수

-  ``tree_learner`` :raw-html:`<a id="tree_learner" title="Permalink to this parameter" href="#tree_learner">&#x1F517;&#xFE0E;</a>`, 기본값 = ``serial``, 타입 = 열거형, 옵션: ``serial``, ``feature``, ``data``, ``voting``, 별칭: ``tree``, ``tree_type``, ``tree_learner_type``

   -  ``serial``, 단일 머신 트리 학습기(Single Machine Tree Learner)

   -  ``feature``, 변수 기반의 병렬 트리 학습기(Feature Parallel Tree Learner), 별칭: ``feature_parallel``

   -  ``data``, 데이터 기반의 병렬 트리 학습기(Data Parallel Tree Learner), 별칭: ``data_parallel``

   -  ``voting``, 투표 기반의 병렬 트리 학습기(Voting Parallel Tree Learner), 별칭: ``voting_parallel``

   -  자세한 내용은 `Distributed Learning Guide <./Parallel-Learning-Guide.rst>`__ 을 참고하세요.

-  ``num_threads`` :raw-html:`<a id="num_threads" title="Permalink to this parameter" href="#num_threads">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0``, 타입 = 정수형, 별칭: ``num_thread``, ``nthread``, ``nthreads``, ``n_jobs``

   -  LightGBM에 사용되는 스레드 수

   -  ``0`` 은 OpenMP의 기본 스레드 수를 의미합니다.

   -  최상의 속도를 위해, 스레드 수가 아닌 **실제 CPU 코어 수** 로 설정하세요. (대부분의 CPU는 `하이퍼 스레딩 <https://en.wikipedia.org/wiki/Hyper-threading>`__ 을 활용해 CPU 코어당 2개의 스레드를 만듭니다.)

   -  데이터셋이 작을 경우 너무 큰 값을 설정하지 마십시오. (예를 들어, 10,000개의 행이 있는 데이터셋에 64개의 스레드를 사용하지 마십시오.)

   -  작업 관리자나 CPU 모니터링 도구가 CPU 코어가 완전히 활용되지 않는다고 보고할 수 있습니다. **이는 정상입니다.**

   -  분산 학습 시, 네트워크 통신 성능을 저하시키므로 CPU 코어를 모두 사용하지 마십시오.

   -  **참고**: 학습 중에 이 값을 변경하지 **마십시오**. 특히 외부 패키지로 여러 작업을 동시에 실행하는 경우 원치 않는 오류가 발생할 수 있습니다.

-  ``device_type`` :raw-html:`<a id="device_type" title="Permalink to this parameter" href="#device_type">&#x1F517;&#xFE0E;</a>`, 기본값 = ``cpu``, 타입 = 열거형, 옵션: ``cpu``, ``gpu``, ``cuda``, 별칭: ``device``

   -  트리 학습을 위한 장치 설정, GPU를 사용하여 더 빠르게 학습할 수 있습니다.

   -  **참고**: 속도를 높이려면 작은 ``max_bin`` (예: 63)을 사용하는 것이 좋습니다.

   -  **참고**: 빠른 속도를 위해 GPU는 기본적으로 32비트 부동 소숫점을 사용하여 합산하므로 일부 작업의 정확도에 영향을 줄 수 있습니다. 64비트 부동 소숫점을 사용하도록 ``gpu_use_dp=true`` 를 설정할 수 있지만, 학습 속도가 느려질 것입니다.

   -  **참고**: GPU를 지원하는 LightGBM을 빌드하려면 `설치 가이드 <./Installation-Guide.rst#build-gpu-version>`__ 를 참고하세요.

-  ``seed`` :raw-html:`<a id="seed" title="Permalink to this parameter" href="#seed">&#x1F517;&#xFE0E;</a>`, 기본값 = ``None``, 타입 = 정수형, 별칭: ``random_seed``, ``random_state``

   -  이 시드(seed)는 다른 시드를 만드는데에 사용됩니다. (예: ``data_random_seed``, ``feature_fraction_seed`` 등)

   -  기본적으로 이 시드는 다른 시드의 기본값보다 우선하여 사용되지 않습니다.

   -  이 시드는 다른 시드보다 우선 순위가 낮으므로, 다른 시드가 명시적으로 설정될 경우 재정의(overiding)됩니다.

-  ``deterministic`` :raw-html:`<a id="deterministic" title="Permalink to this parameter" href="#deterministic">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = 부울

   -  ``cpu`` 장치 유형에서만 사용됩니다.

   -  이를 ``true`` 로 설정하면 동일한 데이터와 동일한 파라미터(와 다른 ``num_threads``)를 사용할 때 안정적인 결과를 얻을 수 있습니다.

   -  다른 시드(seed), 다른 버전의 LightGBM, 다른 컴파일러로 컴파일 된 바이너리, 다른 시스템을 사용할 경우, 결과가 달라질 수 있습니다.

   -  불안정한 결과를 발견할 경우 LightGBM 깃허브 레포지토리에서 `문제 제기 <https://github.com/microsoft/LightGBM/issues>`__ 를 할 수 있습니다.

   -  **참고**: ``true`` 로 설정하면 훈련 속도가 느려질 수 있습니다.

   -  **참고**: ``deterministic=true`` 로 설정한 경우, 수치적(numerical) 문제로 인한 잠재적 불안정성을 피하려면 ``force_col_wise=true`` 나 ``force_row_wise=true`` 를 설정하십시오.

학습 제어 파라미터
---------------------------

-  ``force_col_wise`` :raw-html:`<a id="force_col_wise" title="Permalink to this parameter" href="#force_col_wise">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = 부울

   -  ``cpu`` 장치 유형에서만 사용됩니다.

   -  행 기반의 히스토그램(col-wise histogram)을 강제하려면 이 값을 ``true`` 로 설정하세요.

   -  다음과 같은 경우에 이 값의 사용을 권합니다.

      -  컬럼 개수가 많거나, 총 구간(bins)의 개수가 많은 경우

      -  ``num_threads`` 가 큰 경우 (예: 20보다 큰 경우)

      -  메모리 비용을 줄이고 싶은 경우

   -  **참고**: ``force_col_wise`` 와 ``force_row_wise`` 가 ``false`` 일 경우, LightGBM은 처음에 둘 다 실행하고, 그 후에 더 빠른것을 사용합니다. 테스트 셋의 오버헤드를 제거하려면 더 빠른 것을 직접 ``true`` 로 설정하세요.

   -  **참고**: 이 파라미터는 ``force_row_wise`` 와 동시에 사용할 수 없으므로 둘 중 하나만 사용하십시오.

-  ``force_row_wise`` :raw-html:`<a id="force_row_wise" title="Permalink to this parameter" href="#force_row_wise">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  ``cpu`` 장치 유형에서만 사용됩니다.

   -  열 기반의 히스토그램(row-wise histogram)을 강제하려면 이 값을 ``true`` 로 설정하세요.

   -  다음과 같은 경우에 이 값의 사용을 권합니다.

      -  데이터 개수가 많고 총 구간(bins)의 개수가 상대적으로 적은 경우

      -  ``num_threads`` 가 상대적으로 작은 경우 (예: 16보다 작거나 같은 경우)

      -  속도를 위해 작은 ``bagging_fraction`` 이나 ``goss`` 부스팅을 사용하고자 하는 경우

   -  **참고**: 이를 ``true`` 로 설정하면 Dataset 오브젝트의 메모리 비용이 두 배로 증가합니다. 메모리가 충분하지 않은 경우 ``force_col_wise=true`` 를 설정할 수 있습니다.

   -  **참고**: ``force_col_wise`` 과 ``force_row_wise`` 가 ``false`` 일 경우, LightGBM은 처음에 둘 다 실행하고, 그 후에 더 빠른것을 사용합니다. 테스트 셋의 오버헤드를 제거하려면 더 빠른 것을 직접 ``true`` 로 설정하세요.

   -  **참고**: 이 파라미터는 ``force_col_wise`` 와 동시에 사용할 수 없으므로 둘 중 하나만 사용하십시오.

-  ``histogram_pool_size`` :raw-html:`<a id="histogram_pool_size" title="Permalink to this parameter" href="#histogram_pool_size">&#x1F517;&#xFE0E;</a>`, 기본값 = ``-1.0``, 타입 = 부동 소숫점(double), 별칭: ``hist_pool_size``

   -  기록 히스토그램(historical histogram)의 최대 캐시 크기 (단위: MB)

   -  ``< 0`` 은 제한이 없음을 의미합니다.

-  ``max_depth`` :raw-html:`<a id="max_depth" title="Permalink to this parameter" href="#max_depth">&#x1F517;&#xFE0E;</a>`, 기본값 = ``-1``, 타입 = 정수형

   -  트리 모델의 최대 깊이를 제한합니다. 이는 ``#data`` 가 작을 때, 과적합(over-fitting)을 처리하기 위해 사용됩니다. 그럼에도 트리는 리프 (leaf-wise) 방식으로 확장합니다.

   -  ``<= 0`` 은 제한이 없음을 의미합니다.

-  ``min_data_in_leaf`` :raw-html:`<a id="min_data_in_leaf" title="Permalink to this parameter" href="#min_data_in_leaf">&#x1F517;&#xFE0E;</a>`, 기본값 = ``20``, 타입 = 정수형, 별칭: ``min_data_per_leaf``, ``min_data``, ``min_child_samples``, ``min_samples_leaf``, 제약 조건: ``min_data_in_leaf >= 0``

   -  한 리프(leaf)의 최소 데이터 수. 과적합(over-fitting)을 처리하기 위해 사용됩니다.

   -  **참고**: 이는 헤시안(the Hessian) 기반의 근사치이므로, 때때로 이 값보다 적은 수의 데이터를 갖는 리프 노드를 생성하는 일이 발생할 수 있습니다.

-  ``min_sum_hessian_in_leaf`` :raw-html:`<a id="min_sum_hessian_in_leaf" title="Permalink to this parameter" href="#min_sum_hessian_in_leaf">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1e-3``, 타입 = 부동 소숫점(double), 별칭: ``min_sum_hessian_per_leaf``, ``min_sum_hessian``, ``min_hessian``, ``min_child_weight``, 제약 조건: ``min_sum_hessian_in_leaf >= 0.0``

   -  한 리프(leaf)의 최소 헤시안 합. ``min_data_in_leaf`` 와 동일하게, 과적합(over-fitting)을 처리하기 위해 사용됩니다.

-  ``bagging_fraction`` :raw-html:`<a id="bagging_fraction" title="Permalink to this parameter" href="#bagging_fraction">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.0``, 타입 = 부동 소숫점(double), 별칭: ``sub_row``, ``subsample``, ``bagging``, 제약 조건: ``0.0 < bagging_fraction <= 1.0``

   -  ``feature_fraction`` 과 비슷하지만, 리샘플링(resampling) 없이 데이터의 일부를 무작위로 선택합니다.

   -  학습 속도를 높이기 위해 사용됩니다.

   -  과적합(over-fitting)을 방지하기 위해 사용됩니다.

   -  **참고**: 배깅(bagging)을 활성화하려면, ``bagging_freq`` 도 0이 아닌 값으로 설정해야합니다.

-  ``pos_bagging_fraction`` :raw-html:`<a id="pos_bagging_fraction" title="Permalink to this parameter" href="#pos_bagging_fraction">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.0``, 타입 = 부동 소숫점(double), 별칭: ``pos_sub_row``, ``pos_subsample``, ``pos_bagging``, 제약 조건: ``0.0 < pos_bagging_fraction <= 1.0``

   -  used only in ``binary`` application

   -  used for imbalanced binary classification problem, will randomly sample ``#pos_samples * pos_bagging_fraction`` positive samples in bagging

   -  should be used together with ``neg_bagging_fraction``

   -  set this to ``1.0`` to disable

   -  **참고**: to enable this, you need to set ``bagging_freq`` and ``neg_bagging_fraction`` as well

   -  **참고**: if both ``pos_bagging_fraction`` and ``neg_bagging_fraction`` are set to ``1.0``,  balanced bagging is disabled

   -  **참고**: if balanced bagging is enabled, ``bagging_fraction`` will be ignored

-  ``neg_bagging_fraction`` :raw-html:`<a id="neg_bagging_fraction" title="Permalink to this parameter" href="#neg_bagging_fraction">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.0``, 타입 = 부동 소숫점(double), 별칭: ``neg_sub_row``, ``neg_subsample``, ``neg_bagging``, 제약 조건: ``0.0 < neg_bagging_fraction <= 1.0``

   -  used only in ``binary`` application

   -  used for imbalanced binary classification problem, will randomly sample ``#neg_samples * neg_bagging_fraction`` negative samples in bagging

   -  should be used together with ``pos_bagging_fraction``

   -  set this to ``1.0`` to disable

   -  **참고**: to enable this, you need to set ``bagging_freq`` and ``pos_bagging_fraction`` as well

   -  **참고**: if both ``pos_bagging_fraction`` and ``neg_bagging_fraction`` are set to ``1.0``,  balanced bagging is disabled

   -  **참고**: if balanced bagging is enabled, ``bagging_fraction`` will be ignored

-  ``bagging_freq`` :raw-html:`<a id="bagging_freq" title="Permalink to this parameter" href="#bagging_freq">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0``, 타입 = 정수형, 별칭: ``subsample_freq``

   -  frequency for bagging

   -  ``0`` means disable bagging; ``k`` means perform bagging at every ``k`` iteration. Every ``k``-th iteration, LightGBM will randomly select ``bagging_fraction * 100 %`` of the data to use for the next ``k`` iterations

   -  **참고**: to enable bagging, ``bagging_fraction`` should be set to value smaller than ``1.0`` as well

-  ``bagging_seed`` :raw-html:`<a id="bagging_seed" title="Permalink to this parameter" href="#bagging_seed">&#x1F517;&#xFE0E;</a>`, 기본값 = ``3``, 타입 = 정수형, 별칭: ``bagging_fraction_seed``

   -  random seed for bagging

-  ``feature_fraction`` :raw-html:`<a id="feature_fraction" title="Permalink to this parameter" href="#feature_fraction">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.0``, 타입 = 부동 소숫점(double), 별칭: ``sub_feature``, ``colsample_bytree``, 제약 조건: ``0.0 < feature_fraction <= 1.0``

   -  LightGBM will randomly select a subset of features on each iteration (tree) if ``feature_fraction`` is smaller than ``1.0``. For example, if you set it to ``0.8``, LightGBM will select 80% of features before training each tree

   -  can be used to speed up training

   -  can be used to deal with over-fitting

-  ``feature_fraction_bynode`` :raw-html:`<a id="feature_fraction_bynode" title="Permalink to this parameter" href="#feature_fraction_bynode">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.0``, 타입 = 부동 소숫점(double), 별칭: ``sub_feature_bynode``, ``colsample_bynode``, 제약 조건: ``0.0 < feature_fraction_bynode <= 1.0``

   -  LightGBM will randomly select a subset of features on each tree node if ``feature_fraction_bynode`` is smaller than ``1.0``. For example, if you set it to ``0.8``, LightGBM will select 80% of features at each tree node

   -  can be used to deal with over-fitting

   -  **참고**: unlike ``feature_fraction``, this cannot speed up training

   -  **참고**: if both ``feature_fraction`` and ``feature_fraction_bynode`` are smaller than ``1.0``, the final fraction of each node is ``feature_fraction * feature_fraction_bynode``

-  ``feature_fraction_seed`` :raw-html:`<a id="feature_fraction_seed" title="Permalink to this parameter" href="#feature_fraction_seed">&#x1F517;&#xFE0E;</a>`, 기본값 = ``2``, 타입 = 정수형

   -  random seed for ``feature_fraction``

-  ``extra_trees`` :raw-html:`<a id="extra_trees" title="Permalink to this parameter" href="#extra_trees">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``extra_tree``

   -  use extremely randomized trees

   -  if set to ``true``, when evaluating node splits LightGBM will check only one randomly-chosen threshold for each feature

   -  can be used to speed up training

   -  can be used to deal with over-fitting

-  ``extra_seed`` :raw-html:`<a id="extra_seed" title="Permalink to this parameter" href="#extra_seed">&#x1F517;&#xFE0E;</a>`, 기본값 = ``6``, 타입 = 정수형

   -  random seed for selecting thresholds when ``extra_trees`` is true

-  ``early_stopping_round`` :raw-html:`<a id="early_stopping_round" title="Permalink to this parameter" href="#early_stopping_round">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0``, 타입 = 정수형, 별칭: ``early_stopping_rounds``, ``early_stopping``, ``n_iter_no_change``

   -  will stop training if one metric of one validation data doesn't improve in last ``early_stopping_round`` rounds

   -  ``<= 0`` means disable

   -  can be used to speed up training

-  ``first_metric_only`` :raw-html:`<a id="first_metric_only" title="Permalink to this parameter" href="#first_metric_only">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  LightGBM allows you to provide multiple evaluation metrics. Set this to ``true``, if you want to use only the first metric for early stopping

-  ``max_delta_step`` :raw-html:`<a id="max_delta_step" title="Permalink to this parameter" href="#max_delta_step">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.0``, 타입 = 부동 소숫점(double), 별칭: ``max_tree_output``, ``max_leaf_output``

   -  used to limit the max output of tree leaves

   -  ``<= 0`` means no constraint

   -  the final max output of leaves is ``learning_rate * max_delta_step``

-  ``lambda_l1`` :raw-html:`<a id="lambda_l1" title="Permalink to this parameter" href="#lambda_l1">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.0``, 타입 = 부동 소숫점(double), 별칭: ``reg_alpha``, ``l1_regularization``, 제약 조건: ``lambda_l1 >= 0.0``

   -  L1 regularization

-  ``lambda_l2`` :raw-html:`<a id="lambda_l2" title="Permalink to this parameter" href="#lambda_l2">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.0``, 타입 = 부동 소숫점(double), 별칭: ``reg_lambda``, ``lambda``, ``l2_regularization``, 제약 조건: ``lambda_l2 >= 0.0``

   -  L2 regularization

-  ``linear_lambda`` :raw-html:`<a id="linear_lambda" title="Permalink to this parameter" href="#linear_lambda">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.0``, 타입 = 부동 소숫점(double), 제약 조건: ``linear_lambda >= 0.0``

   -  linear tree regularization, corresponds to the parameter ``lambda`` in Eq. 3 of `Gradient Boosting with Piece-Wise Linear Regression Trees <https://arxiv.org/pdf/1802.05640.pdf>`__

-  ``min_gain_to_split`` :raw-html:`<a id="min_gain_to_split" title="Permalink to this parameter" href="#min_gain_to_split">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.0``, 타입 = 부동 소숫점(double), 별칭: ``min_split_gain``, 제약 조건: ``min_gain_to_split >= 0.0``

   -  the minimal gain to perform split

   -  can be used to speed up training

-  ``drop_rate`` :raw-html:`<a id="drop_rate" title="Permalink to this parameter" href="#drop_rate">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.1``, 타입 = 부동 소숫점(double), 별칭: ``rate_drop``, 제약 조건: ``0.0 <= drop_rate <= 1.0``

   -  used only in ``dart``

   -  dropout rate: a fraction of previous trees to drop during the dropout

-  ``max_drop`` :raw-html:`<a id="max_drop" title="Permalink to this parameter" href="#max_drop">&#x1F517;&#xFE0E;</a>`, 기본값 = ``50``, 타입 = 정수형

   -  used only in ``dart``

   -  max number of dropped trees during one boosting iteration

   -  ``<=0`` means no limit

-  ``skip_drop`` :raw-html:`<a id="skip_drop" title="Permalink to this parameter" href="#skip_drop">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.5``, 타입 = 부동 소숫점(double), 제약 조건: ``0.0 <= skip_drop <= 1.0``

   -  used only in ``dart``

   -  probability of skipping the dropout procedure during a boosting iteration

-  ``xgboost_dart_mode`` :raw-html:`<a id="xgboost_dart_mode" title="Permalink to this parameter" href="#xgboost_dart_mode">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  used only in ``dart``

   -  set this to ``true``, if you want to use xgboost dart mode

-  ``uniform_drop`` :raw-html:`<a id="uniform_drop" title="Permalink to this parameter" href="#uniform_drop">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  used only in ``dart``

   -  set this to ``true``, if you want to use uniform drop

-  ``drop_seed`` :raw-html:`<a id="drop_seed" title="Permalink to this parameter" href="#drop_seed">&#x1F517;&#xFE0E;</a>`, 기본값 = ``4``, 타입 = 정수형

   -  used only in ``dart``

   -  random seed to choose dropping models

-  ``top_rate`` :raw-html:`<a id="top_rate" title="Permalink to this parameter" href="#top_rate">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.2``, 타입 = 부동 소숫점(double), 제약 조건: ``0.0 <= top_rate <= 1.0``

   -  used only in ``goss``

   -  the retain ratio of large gradient data

-  ``other_rate`` :raw-html:`<a id="other_rate" title="Permalink to this parameter" href="#other_rate">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.1``, 타입 = 부동 소숫점(double), 제약 조건: ``0.0 <= other_rate <= 1.0``

   -  used only in ``goss``

   -  the retain ratio of small gradient data

-  ``min_data_per_group`` :raw-html:`<a id="min_data_per_group" title="Permalink to this parameter" href="#min_data_per_group">&#x1F517;&#xFE0E;</a>`, 기본값 = ``100``, 타입 = 정수형, 제약 조건: ``min_data_per_group > 0``

   -  minimal number of data per categorical group

-  ``max_cat_threshold`` :raw-html:`<a id="max_cat_threshold" title="Permalink to this parameter" href="#max_cat_threshold">&#x1F517;&#xFE0E;</a>`, 기본값 = ``32``, 타입 = 정수형, 제약 조건: ``max_cat_threshold > 0``

   -  used for the categorical features

   -  limit number of split points considered for categorical features. See `the documentation on how LightGBM finds optimal splits for categorical features <./Features.rst#optimal-split-for-categorical-features>`_ for more details

   -  can be used to speed up training

-  ``cat_l2`` :raw-html:`<a id="cat_l2" title="Permalink to this parameter" href="#cat_l2">&#x1F517;&#xFE0E;</a>`, 기본값 = ``10.0``, 타입 = 부동 소숫점(double), 제약 조건: ``cat_l2 >= 0.0``

   -  used for the categorical features

   -  L2 regularization in categorical split

-  ``cat_smooth`` :raw-html:`<a id="cat_smooth" title="Permalink to this parameter" href="#cat_smooth">&#x1F517;&#xFE0E;</a>`, 기본값 = ``10.0``, 타입 = 부동 소숫점(double), 제약 조건: ``cat_smooth >= 0.0``

   -  used for the categorical features

   -  this can reduce the effect of noises in categorical features, especially for categories with few data

-  ``max_cat_to_onehot`` :raw-html:`<a id="max_cat_to_onehot" title="Permalink to this parameter" href="#max_cat_to_onehot">&#x1F517;&#xFE0E;</a>`, 기본값 = ``4``, 타입 = 정수형, 제약 조건: ``max_cat_to_onehot > 0``

   -  when number of categories of one feature smaller than or equal to ``max_cat_to_onehot``, one-vs-other split algorithm will be used

-  ``top_k`` :raw-html:`<a id="top_k" title="Permalink to this parameter" href="#top_k">&#x1F517;&#xFE0E;</a>`, 기본값 = ``20``, 타입 = 정수형, 별칭: ``topk``, 제약 조건: ``top_k > 0``

   -  used only in ``voting`` tree learner, refer to `Voting parallel <./Parallel-Learning-Guide.rst#choose-appropriate-parallel-algorithm>`__

   -  set this to larger value for more accurate result, but it will slow down the training speed

-  ``monotone_constraints`` :raw-html:`<a id="monotone_constraints" title="Permalink to this parameter" href="#monotone_constraints">&#x1F517;&#xFE0E;</a>`, 기본값 = ``None``, 타입 = multi-int, 별칭: ``mc``, ``monotone_constraint``, ``monotonic_cst``

   -  used for constraints of monotonic features

   -  ``1`` means increasing, ``-1`` means decreasing, ``0`` means non-constraint

   -  you need to specify all features in order. For example, ``mc=-1,0,1`` means decreasing for 1st feature, non-constraint for 2nd feature and increasing for the 3rd feature

-  ``monotone_constraints_method`` :raw-html:`<a id="monotone_constraints_method" title="Permalink to this parameter" href="#monotone_constraints_method">&#x1F517;&#xFE0E;</a>`, 기본값 = ``basic``, 타입 = 열거형, 옵션: ``basic``, ``intermediate``, ``advanced``, 별칭: ``monotone_constraining_method``, ``mc_method``

   -  used only if ``monotone_constraints`` is set

   -  monotone constraints method

      -  ``basic``, the most basic monotone constraints method. It does not slow the library at all, but over-constrains the predictions

      -  ``intermediate``, a `more advanced method <https://hal.archives-ouvertes.fr/hal-02862802/document>`__, which may slow the library very slightly. However, this method is much less constraining than the basic method and should significantly improve the results

      -  ``advanced``, an `even more advanced method <https://hal.archives-ouvertes.fr/hal-02862802/document>`__, which may slow the library. However, this method is even less constraining than the intermediate method and should again significantly improve the results

-  ``monotone_penalty`` :raw-html:`<a id="monotone_penalty" title="Permalink to this parameter" href="#monotone_penalty">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.0``, 타입 = 부동 소숫점(double), 별칭: ``monotone_splits_penalty``, ``ms_penalty``, ``mc_penalty``, 제약 조건: ``monotone_penalty >= 0.0``

   -  used only if ``monotone_constraints`` is set

   -  `monotone penalty <https://hal.archives-ouvertes.fr/hal-02862802/document>`__: a penalization parameter X forbids any monotone splits on the first X (rounded down) level(s) of the tree. The penalty applied to monotone splits on a given depth is a continuous, increasing function the penalization parameter

   -  if ``0.0`` (the default), no penalization is applied

-  ``feature_contri`` :raw-html:`<a id="feature_contri" title="Permalink to this parameter" href="#feature_contri">&#x1F517;&#xFE0E;</a>`, 기본값 = ``None``, 타입 = multi-double, 별칭: ``feature_contrib``, ``fc``, ``fp``, ``feature_penalty``

   -  used to control feature's split gain, will use ``gain[i] = max(0, feature_contri[i]) * gain[i]`` to replace the split gain of i-th feature

   -  you need to specify all features in order

-  ``forcedsplits_filename`` :raw-html:`<a id="forcedsplits_filename" title="Permalink to this parameter" href="#forcedsplits_filename">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열, 별칭: ``fs``, ``forced_splits_filename``, ``forced_splits_file``, ``forced_splits``

   -  path to a ``.json`` file that specifies splits to force at the top of every decision tree before best-first learning commences

   -  ``.json`` file can be arbitrarily nested, and each split contains ``feature``, ``threshold`` fields, as well as ``left`` and ``right`` fields representing subsplits

   -  categorical splits are forced in a one-hot fashion, with ``left`` representing the split containing the feature value and ``right`` representing other values

   -  **참고**: the forced split logic will be ignored, if the split makes gain worse

   -  see `this file <https://github.com/microsoft/LightGBM/tree/master/examples/binary_classification/forced_splits.json>`__ as an example

-  ``refit_decay_rate`` :raw-html:`<a id="refit_decay_rate" title="Permalink to this parameter" href="#refit_decay_rate">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.9``, 타입 = 부동 소숫점(double), 제약 조건: ``0.0 <= refit_decay_rate <= 1.0``

   -  decay rate of ``refit`` task, will use ``leaf_output = refit_decay_rate * old_leaf_output + (1.0 - refit_decay_rate) * new_leaf_output`` to refit trees

   -  used only in ``refit`` task in CLI version or as argument in ``refit`` function in language-specific package

-  ``cegb_tradeoff`` :raw-html:`<a id="cegb_tradeoff" title="Permalink to this parameter" href="#cegb_tradeoff">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.0``, 타입 = 부동 소숫점(double), 제약 조건: ``cegb_tradeoff >= 0.0``

   -  cost-effective gradient boosting multiplier for all penalties

-  ``cegb_penalty_split`` :raw-html:`<a id="cegb_penalty_split" title="Permalink to this parameter" href="#cegb_penalty_split">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.0``, 타입 = 부동 소숫점(double), 제약 조건: ``cegb_penalty_split >= 0.0``

   -  cost-effective gradient-boosting penalty for splitting a node

-  ``cegb_penalty_feature_lazy`` :raw-html:`<a id="cegb_penalty_feature_lazy" title="Permalink to this parameter" href="#cegb_penalty_feature_lazy">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0,0,...,0``, 타입 = multi-double

   -  cost-effective gradient boosting penalty for using a feature

   -  applied per data point

-  ``cegb_penalty_feature_coupled`` :raw-html:`<a id="cegb_penalty_feature_coupled" title="Permalink to this parameter" href="#cegb_penalty_feature_coupled">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0,0,...,0``, 타입 = multi-double

   -  cost-effective gradient boosting penalty for using a feature

   -  applied once per forest

-  ``path_smooth`` :raw-html:`<a id="path_smooth" title="Permalink to this parameter" href="#path_smooth">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0``, 타입 = 부동 소숫점(double), 제약 조건: ``path_smooth >=  0.0``

   -  controls smoothing applied to tree nodes

   -  helps prevent overfitting on leaves with few samples

   -  if set to zero, no smoothing is applied

   -  if ``path_smooth > 0`` then ``min_data_in_leaf`` must be at least ``2``

   -  larger values give stronger regularization

      -  the weight of each node is ``(n / path_smooth) * w + w_p / (n / path_smooth + 1)``, where ``n`` is the number of samples in the node, ``w`` is the optimal node weight to minimise the loss (approximately ``-sum_gradients / sum_hessians``), and ``w_p`` is the weight of the parent node

      -  note that the parent output ``w_p`` itself has smoothing applied, unless it is the root node, so that the smoothing effect accumulates with the tree depth

-  ``interaction_constraints`` :raw-html:`<a id="interaction_constraints" title="Permalink to this parameter" href="#interaction_constraints">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열

   -  controls which features can appear in the same branch

   -  by default interaction constraints are disabled, to enable them you can specify

      -  for CLI, lists separated by commas, e.g. ``[0,1,2],[2,3]``

      -  for Python-package, list of lists, e.g. ``[[0, 1, 2], [2, 3]]``

      -  for R-package, list of character or numeric vectors, e.g. ``list(c("var1", "var2", "var3"), c("var3", "var4"))`` or ``list(c(1L, 2L, 3L), c(3L, 4L))``. Numeric vectors should use 1-based indexing, where ``1L`` is the first feature, ``2L`` is the second feature, etc

   -  any two features can only appear in the same branch only if there exists a constraint containing both features

-  ``verbosity`` :raw-html:`<a id="verbosity" title="Permalink to this parameter" href="#verbosity">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1``, 타입 = 정수형, 별칭: ``verbose``

   -  controls the level of LightGBM's verbosity

   -  ``< 0``: Fatal, ``= 0``: Error (Warning), ``= 1``: Info, ``> 1``: Debug

-  ``input_model`` :raw-html:`<a id="input_model" title="Permalink to this parameter" href="#input_model">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열, 별칭: ``model_input``, ``model_in``

   -  filename of input model

   -  for ``prediction`` task, this model will be applied to prediction data

   -  for ``train`` task, training will be continued from this model

   -  **참고**: CLI 버전에서만 사용 가능합니다.

-  ``output_model`` :raw-html:`<a id="output_model" title="Permalink to this parameter" href="#output_model">&#x1F517;&#xFE0E;</a>`, 기본값 = ``LightGBM_model.txt``, 타입 = 문자열, 별칭: ``model_output``, ``model_out``

   -  filename of output model in training

   -  **참고**: CLI 버전에서만 사용 가능합니다.

-  ``saved_feature_importance_type`` :raw-html:`<a id="saved_feature_importance_type" title="Permalink to this parameter" href="#saved_feature_importance_type">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0``, 타입 = 정수형

   -  the feature importance type in the saved model file

   -  ``0``: count-based feature importance (numbers of splits are counted); ``1``: gain-based feature importance (values of gain are counted)

   -  **참고**: CLI 버전에서만 사용 가능합니다.

-  ``snapshot_freq`` :raw-html:`<a id="snapshot_freq" title="Permalink to this parameter" href="#snapshot_freq">&#x1F517;&#xFE0E;</a>`, 기본값 = ``-1``, 타입 = 정수형, 별칭: ``save_period``

   -  frequency of saving model file snapshot

   -  set this to positive value to enable this function. For example, the model file will be snapshotted at each iteration if ``snapshot_freq=1``

   -  **참고**: CLI 버전에서만 사용 가능합니다.

IO Parameters
-------------

Dataset Parameters
~~~~~~~~~~~~~~~~~~

-  ``linear_tree`` :raw-html:`<a id="linear_tree" title="Permalink to this parameter" href="#linear_tree">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``linear_trees``

   -  fit piecewise linear gradient boosting tree

      -  tree splits are chosen in the usual way, but the model at each leaf is linear instead of constant

      -  the linear model at each leaf includes all the numerical features in that leaf's branch

      -  categorical features are used for splits as normal but are not used in the linear models

      -  missing values should not be encoded as ``0``. Use ``np.nan`` for Python, ``NA`` for the CLI, and ``NA``, ``NA_real_``, or ``NA_integer_`` for R

      -  it is recommended to rescale data before training so that features have similar mean and standard deviation

      -  **참고**: only works with CPU and ``serial`` tree learner

      -  **참고**: ``regression_l1`` objective is not supported with linear tree boosting

      -  **참고**: setting ``linear_tree=true`` significantly increases the memory use of LightGBM

      -  **참고**: if you specify ``monotone_constraints``, constraints will be enforced when choosing the split points, but not when fitting the linear models on leaves

-  ``max_bin`` :raw-html:`<a id="max_bin" title="Permalink to this parameter" href="#max_bin">&#x1F517;&#xFE0E;</a>`, 기본값 = ``255``, 타입 = 정수형, 별칭: ``max_bins``, 제약 조건: ``max_bin > 1``

   -  max number of bins that feature values will be bucketed in

   -  small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)

   -  LightGBM will auto compress memory according to ``max_bin``. For example, LightGBM will use ``uint8_t`` for feature value if ``max_bin=255``

-  ``max_bin_by_feature`` :raw-html:`<a id="max_bin_by_feature" title="Permalink to this parameter" href="#max_bin_by_feature">&#x1F517;&#xFE0E;</a>`, 기본값 = ``None``, 타입 = multi-int

   -  max number of bins for each feature

   -  if not specified, will use ``max_bin`` for all features

-  ``min_data_in_bin`` :raw-html:`<a id="min_data_in_bin" title="Permalink to this parameter" href="#min_data_in_bin">&#x1F517;&#xFE0E;</a>`, 기본값 = ``3``, 타입 = 정수형, 제약 조건: ``min_data_in_bin > 0``

   -  minimal number of data inside one bin

   -  use this to avoid one-data-one-bin (potential over-fitting)

-  ``bin_construct_sample_cnt`` :raw-html:`<a id="bin_construct_sample_cnt" title="Permalink to this parameter" href="#bin_construct_sample_cnt">&#x1F517;&#xFE0E;</a>`, 기본값 = ``200000``, 타입 = 정수형, 별칭: ``subsample_for_bin``, 제약 조건: ``bin_construct_sample_cnt > 0``

   -  number of data that sampled to construct feature discrete bins

   -  setting this to larger value will give better training result, but may increase data loading time

   -  set this to larger value if data is very sparse

   -  **참고**: don't set this to small values, otherwise, you may encounter unexpected errors and poor accuracy

-  ``data_random_seed`` :raw-html:`<a id="data_random_seed" title="Permalink to this parameter" href="#data_random_seed">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1``, 타입 = 정수형, 별칭: ``data_seed``

   -  random seed for sampling data to construct histogram bins

-  ``is_enable_sparse`` :raw-html:`<a id="is_enable_sparse" title="Permalink to this parameter" href="#is_enable_sparse">&#x1F517;&#xFE0E;</a>`, 기본값 = ``true``, 타입 = bool, 별칭: ``is_sparse``, ``enable_sparse``, ``sparse``

   -  used to enable/disable sparse optimization

-  ``enable_bundle`` :raw-html:`<a id="enable_bundle" title="Permalink to this parameter" href="#enable_bundle">&#x1F517;&#xFE0E;</a>`, 기본값 = ``true``, 타입 = bool, 별칭: ``is_enable_bundle``, ``bundle``

   -  set this to ``false`` to disable Exclusive Feature Bundling (EFB), which is described in `LightGBM: A Highly Efficient Gradient Boosting Decision Tree <https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree>`__

   -  **참고**: disabling this may cause the slow training speed for sparse datasets

-  ``use_missing`` :raw-html:`<a id="use_missing" title="Permalink to this parameter" href="#use_missing">&#x1F517;&#xFE0E;</a>`, 기본값 = ``true``, 타입 = bool

   -  set this to ``false`` to disable the special handle of missing value

-  ``zero_as_missing`` :raw-html:`<a id="zero_as_missing" title="Permalink to this parameter" href="#zero_as_missing">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  set this to ``true`` to treat all zero as missing values (including the unshown values in LibSVM / sparse matrices)

   -  set this to ``false`` to use ``na`` for representing missing values

-  ``feature_pre_filter`` :raw-html:`<a id="feature_pre_filter" title="Permalink to this parameter" href="#feature_pre_filter">&#x1F517;&#xFE0E;</a>`, 기본값 = ``true``, 타입 = bool

   -  set this to ``true`` (the default) to tell LightGBM to ignore the features that are unsplittable based on ``min_data_in_leaf``

   -  as dataset object is initialized only once and cannot be changed after that, you may need to set this to ``false`` when searching parameters with ``min_data_in_leaf``, otherwise features are filtered by ``min_data_in_leaf`` firstly if you don't reconstruct dataset object

   -  **참고**: setting this to ``false`` may slow down the training

-  ``pre_partition`` :raw-html:`<a id="pre_partition" title="Permalink to this parameter" href="#pre_partition">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``is_pre_partition``

   -  used for distributed learning (excluding the ``feature_parallel`` mode)

   -  ``true`` if training data are pre-partitioned, and different machines use different partitions

-  ``two_round`` :raw-html:`<a id="two_round" title="Permalink to this parameter" href="#two_round">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``two_round_loading``, ``use_two_round_loading``

   -  set this to ``true`` if data file is too big to fit in memory

   -  by default, LightGBM will map data file to memory and load features from memory. This will provide faster data loading speed, but may cause run out of memory error when the data file is very big

   -  **참고**: works only in case of loading data directly from text file

-  ``header`` :raw-html:`<a id="header" title="Permalink to this parameter" href="#header">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``has_header``

   -  set this to ``true`` if input data has header

   -  **참고**: works only in case of loading data directly from text file

-  ``label_column`` :raw-html:`<a id="label_column" title="Permalink to this parameter" href="#label_column">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 정수형 or string, 별칭: ``label``

   -  used to specify the label column

   -  use number for index, e.g. ``label=0`` means column\_0 is the label

   -  add a prefix ``name:`` for column name, e.g. ``label=name:is_click``

   -  if omitted, the first column in the training data is used as the label

   -  **참고**: works only in case of loading data directly from text file

-  ``weight_column`` :raw-html:`<a id="weight_column" title="Permalink to this parameter" href="#weight_column">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 정수형 or string, 별칭: ``weight``

   -  used to specify the weight column

   -  use number for index, e.g. ``weight=0`` means column\_0 is the weight

   -  add a prefix ``name:`` for column name, e.g. ``weight=name:weight``

   -  **참고**: works only in case of loading data directly from text file

   -  **참고**: index starts from ``0`` and it doesn't count the label column when passing type is ``int``, e.g. when label is column\_0, and weight is column\_1, the correct parameter is ``weight=0``

-  ``group_column`` :raw-html:`<a id="group_column" title="Permalink to this parameter" href="#group_column">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 정수형 or string, 별칭: ``group``, ``group_id``, ``query_column``, ``query``, ``query_id``

   -  used to specify the query/group id column

   -  use number for index, e.g. ``query=0`` means column\_0 is the query id

   -  add a prefix ``name:`` for column name, e.g. ``query=name:query_id``

   -  **참고**: works only in case of loading data directly from text file

   -  **참고**: data should be grouped by query\_id, for more information, see `Query Data <#query-data>`__

   -  **참고**: index starts from ``0`` and it doesn't count the label column when passing type is ``int``, e.g. when label is column\_0 and query\_id is column\_1, the correct parameter is ``query=0``

-  ``ignore_column`` :raw-html:`<a id="ignore_column" title="Permalink to this parameter" href="#ignore_column">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = multi-int or string, 별칭: ``ignore_feature``, ``blacklist``

   -  used to specify some ignoring columns in training

   -  use number for index, e.g. ``ignore_column=0,1,2`` means column\_0, column\_1 and column\_2 will be ignored

   -  add a prefix ``name:`` for column name, e.g. ``ignore_column=name:c1,c2,c3`` means c1, c2 and c3 will be ignored

   -  **참고**: works only in case of loading data directly from text file

   -  **참고**: index starts from ``0`` and it doesn't count the label column when passing type is ``int``

   -  **참고**: despite the fact that specified columns will be completely ignored during the training, they still should have a valid format allowing LightGBM to load file successfully

-  ``categorical_feature`` :raw-html:`<a id="categorical_feature" title="Permalink to this parameter" href="#categorical_feature">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = multi-int or string, 별칭: ``cat_feature``, ``categorical_column``, ``cat_column``, ``categorical_features``

   -  used to specify categorical features

   -  use number for index, e.g. ``categorical_feature=0,1,2`` means column\_0, column\_1 and column\_2 are categorical features

   -  add a prefix ``name:`` for column name, e.g. ``categorical_feature=name:c1,c2,c3`` means c1, c2 and c3 are categorical features

   -  **참고**: only supports categorical with ``int`` type (not applicable for data represented as pandas DataFrame in Python-package)

   -  **참고**: index starts from ``0`` and it doesn't count the label column when passing type is ``int``

   -  **참고**: all values should be less than ``Int32.MaxValue`` (2147483647)

   -  **참고**: using large values could be memory consuming. Tree decision rule works best when categorical features are presented by consecutive integers starting from zero

   -  **참고**: all negative values will be treated as **missing values**

   -  **참고**: the output cannot be monotonically constrained with respect to a categorical feature

-  ``forcedbins_filename`` :raw-html:`<a id="forcedbins_filename" title="Permalink to this parameter" href="#forcedbins_filename">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열

   -  path to a ``.json`` file that specifies bin upper bounds for some or all features

   -  ``.json`` file should contain an array of objects, each containing the word ``feature`` (integer feature index) and ``bin_upper_bound`` (array of thresholds for binning)

   -  see `this file <https://github.com/microsoft/LightGBM/tree/master/examples/regression/forced_bins.json>`__ as an example

-  ``save_binary`` :raw-html:`<a id="save_binary" title="Permalink to this parameter" href="#save_binary">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``is_save_binary``, ``is_save_binary_file``

   -  if ``true``, LightGBM will save the dataset (including validation data) to a binary file. This speed ups the data loading for the next time

   -  **참고**: ``init_score`` is not saved in binary file

   -  **참고**: CLI 버전에서만 사용 가능합니다.; for language-specific packages you can use the correspondent function

-  ``precise_float_parser`` :raw-html:`<a id="precise_float_parser" title="Permalink to this parameter" href="#precise_float_parser">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  use precise floating point number parsing for text parser (e.g. CSV, TSV, LibSVM input)

   -  **참고**: setting this to ``true`` may lead to much slower text parsing

Predict Parameters
~~~~~~~~~~~~~~~~~~

-  ``start_iteration_predict`` :raw-html:`<a id="start_iteration_predict" title="Permalink to this parameter" href="#start_iteration_predict">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0``, 타입 = 정수형

   -  used only in ``prediction`` task

   -  used to specify from which iteration to start the prediction

   -  ``<= 0`` means from the first iteration

-  ``num_iteration_predict`` :raw-html:`<a id="num_iteration_predict" title="Permalink to this parameter" href="#num_iteration_predict">&#x1F517;&#xFE0E;</a>`, 기본값 = ``-1``, 타입 = 정수형

   -  used only in ``prediction`` task

   -  used to specify how many trained iterations will be used in prediction

   -  ``<= 0`` means no limit

-  ``predict_raw_score`` :raw-html:`<a id="predict_raw_score" title="Permalink to this parameter" href="#predict_raw_score">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``is_predict_raw_score``, ``predict_rawscore``, ``raw_score``

   -  used only in ``prediction`` task

   -  set this to ``true`` to predict only the raw scores

   -  set this to ``false`` to predict transformed scores

-  ``predict_leaf_index`` :raw-html:`<a id="predict_leaf_index" title="Permalink to this parameter" href="#predict_leaf_index">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``is_predict_leaf_index``, ``leaf_index``

   -  used only in ``prediction`` task

   -  set this to ``true`` to predict with leaf index of all trees

-  ``predict_contrib`` :raw-html:`<a id="predict_contrib" title="Permalink to this parameter" href="#predict_contrib">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``is_predict_contrib``, ``contrib``

   -  used only in ``prediction`` task

   -  set this to ``true`` to estimate `SHAP values <https://arxiv.org/abs/1706.06060>`__, which represent how each feature contributes to each prediction

   -  produces ``#features + 1`` values where the last value is the expected value of the model output over the training data

   -  **참고**: if you want to get more explanation for your model's predictions using SHAP values like SHAP interaction values, you can install `shap package <https://github.com/slundberg/shap>`__

   -  **참고**: unlike the shap package, with ``predict_contrib`` we return a matrix with an extra column, where the last column is the expected value

   -  **참고**: this feature is not implemented for linear trees

-  ``predict_disable_shape_check`` :raw-html:`<a id="predict_disable_shape_check" title="Permalink to this parameter" href="#predict_disable_shape_check">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  used only in ``prediction`` task

   -  control whether or not LightGBM raises an error when you try to predict on data with a different number of features than the training data

   -  if ``false`` (the default), a fatal error will be raised if the number of features in the dataset you predict on differs from the number seen during training

   -  if ``true``, LightGBM will attempt to predict on whatever data you provide. This is dangerous because you might get incorrect predictions, but you could use it in situations where it is difficult or expensive to generate some features and you are very confident that they were never chosen for splits in the model

   -  **참고**: be very careful setting this parameter to ``true``

-  ``pred_early_stop`` :raw-html:`<a id="pred_early_stop" title="Permalink to this parameter" href="#pred_early_stop">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  used only in ``prediction`` task

   -  used only in ``classification`` and ``ranking`` applications

   -  if ``true``, will use early-stopping to speed up the prediction. May affect the accuracy

   -  **참고**: cannot be used with ``rf`` boosting type or custom objective function

-  ``pred_early_stop_freq`` :raw-html:`<a id="pred_early_stop_freq" title="Permalink to this parameter" href="#pred_early_stop_freq">&#x1F517;&#xFE0E;</a>`, 기본값 = ``10``, 타입 = 정수형

   -  used only in ``prediction`` task

   -  the frequency of checking early-stopping prediction

-  ``pred_early_stop_margin`` :raw-html:`<a id="pred_early_stop_margin" title="Permalink to this parameter" href="#pred_early_stop_margin">&#x1F517;&#xFE0E;</a>`, 기본값 = ``10.0``, 타입 = 부동 소숫점(double)

   -  used only in ``prediction`` task

   -  the threshold of margin in early-stopping prediction

-  ``output_result`` :raw-html:`<a id="output_result" title="Permalink to this parameter" href="#output_result">&#x1F517;&#xFE0E;</a>`, 기본값 = ``LightGBM_predict_result.txt``, 타입 = 문자열, 별칭: ``predict_result``, ``prediction_result``, ``predict_name``, ``prediction_name``, ``pred_name``, ``name_pred``

   -  used only in ``prediction`` task

   -  filename of prediction result

   -  **참고**: CLI 버전에서만 사용 가능합니다.

Convert Parameters
~~~~~~~~~~~~~~~~~~

-  ``convert_model_language`` :raw-html:`<a id="convert_model_language" title="Permalink to this parameter" href="#convert_model_language">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열

   -  used only in ``convert_model`` task

   -  only ``cpp`` is supported yet; for conversion model to other languages consider using `m2cgen <https://github.com/BayesWitnesses/m2cgen>`__ utility

   -  if ``convert_model_language`` is set and ``task=train``, the model will be also converted

   -  **참고**: CLI 버전에서만 사용 가능합니다.

-  ``convert_model`` :raw-html:`<a id="convert_model" title="Permalink to this parameter" href="#convert_model">&#x1F517;&#xFE0E;</a>`, 기본값 = ``gbdt_prediction.cpp``, 타입 = 문자열, 별칭: ``convert_model_file``

   -  used only in ``convert_model`` task

   -  output filename of converted model

   -  **참고**: CLI 버전에서만 사용 가능합니다.

Objective Parameters
--------------------

-  ``objective_seed`` :raw-html:`<a id="objective_seed" title="Permalink to this parameter" href="#objective_seed">&#x1F517;&#xFE0E;</a>`, 기본값 = ``5``, 타입 = 정수형

   -  used only in ``rank_xendcg`` objective

   -  random seed for objectives, if random process is needed

-  ``num_class`` :raw-html:`<a id="num_class" title="Permalink to this parameter" href="#num_class">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1``, 타입 = 정수형, 별칭: ``num_classes``, 제약 조건: ``num_class > 0``

   -  used only in ``multi-class`` classification application

-  ``is_unbalance`` :raw-html:`<a id="is_unbalance" title="Permalink to this parameter" href="#is_unbalance">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``unbalance``, ``unbalanced_sets``

   -  used only in ``binary`` and ``multiclassova`` applications

   -  set this to ``true`` if training data are unbalanced

   -  **참고**: while enabling this should increase the overall performance metric of your model, it will also result in poor estimates of the individual class probabilities

   -  **참고**: this parameter cannot be used at the same time with ``scale_pos_weight``, choose only **one** of them

-  ``scale_pos_weight`` :raw-html:`<a id="scale_pos_weight" title="Permalink to this parameter" href="#scale_pos_weight">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.0``, 타입 = 부동 소숫점(double), 제약 조건: ``scale_pos_weight > 0.0``

   -  used only in ``binary`` and ``multiclassova`` applications

   -  weight of labels with positive class

   -  **참고**: while enabling this should increase the overall performance metric of your model, it will also result in poor estimates of the individual class probabilities

   -  **참고**: this parameter cannot be used at the same time with ``is_unbalance``, choose only **one** of them

-  ``sigmoid`` :raw-html:`<a id="sigmoid" title="Permalink to this parameter" href="#sigmoid">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.0``, 타입 = 부동 소숫점(double), 제약 조건: ``sigmoid > 0.0``

   -  used only in ``binary`` and ``multiclassova`` classification and in ``lambdarank`` applications

   -  parameter for the sigmoid function

-  ``boost_from_average`` :raw-html:`<a id="boost_from_average" title="Permalink to this parameter" href="#boost_from_average">&#x1F517;&#xFE0E;</a>`, 기본값 = ``true``, 타입 = bool

   -  used only in ``regression``, ``binary``, ``multiclassova`` and ``cross-entropy`` applications

   -  adjusts initial score to the mean of labels for faster convergence

-  ``reg_sqrt`` :raw-html:`<a id="reg_sqrt" title="Permalink to this parameter" href="#reg_sqrt">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  used only in ``regression`` application

   -  used to fit ``sqrt(label)`` instead of original values and prediction result will be also automatically converted to ``prediction^2``

   -  might be useful in case of large-range labels

-  ``alpha`` :raw-html:`<a id="alpha" title="Permalink to this parameter" href="#alpha">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.9``, 타입 = 부동 소숫점(double), 제약 조건: ``alpha > 0.0``

   -  used only in ``huber`` and ``quantile`` ``regression`` applications

   -  parameter for `Huber loss <https://en.wikipedia.org/wiki/Huber_loss>`__ and `Quantile regression <https://en.wikipedia.org/wiki/Quantile_regression>`__

-  ``fair_c`` :raw-html:`<a id="fair_c" title="Permalink to this parameter" href="#fair_c">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.0``, 타입 = 부동 소숫점(double), 제약 조건: ``fair_c > 0.0``

   -  used only in ``fair`` ``regression`` application

   -  parameter for `Fair loss <https://www.kaggle.com/c/allstate-claims-severity/discussion/24520>`__

-  ``poisson_max_delta_step`` :raw-html:`<a id="poisson_max_delta_step" title="Permalink to this parameter" href="#poisson_max_delta_step">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0.7``, 타입 = 부동 소숫점(double), 제약 조건: ``poisson_max_delta_step > 0.0``

   -  used only in ``poisson`` ``regression`` application

   -  parameter for `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__ to safeguard optimization

-  ``tweedie_variance_power`` :raw-html:`<a id="tweedie_variance_power" title="Permalink to this parameter" href="#tweedie_variance_power">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1.5``, 타입 = 부동 소숫점(double), 제약 조건: ``1.0 <= tweedie_variance_power < 2.0``

   -  used only in ``tweedie`` ``regression`` application

   -  used to control the variance of the tweedie distribution

   -  set this closer to ``2`` to shift towards a **Gamma** distribution

   -  set this closer to ``1`` to shift towards a **Poisson** distribution

-  ``lambdarank_truncation_level`` :raw-html:`<a id="lambdarank_truncation_level" title="Permalink to this parameter" href="#lambdarank_truncation_level">&#x1F517;&#xFE0E;</a>`, 기본값 = ``30``, 타입 = 정수형, 제약 조건: ``lambdarank_truncation_level > 0``

   -  used only in ``lambdarank`` application

   -  controls the number of top-results to focus on during training, refer to "truncation level" in the Sec. 3 of `LambdaMART paper <https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf>`__

   -  this parameter is closely related to the desirable cutoff ``k`` in the metric **NDCG@k** that we aim at optimizing the ranker for. The optimal setting for this parameter is likely to be slightly higher than ``k`` (e.g., ``k + 3``) to include more pairs of documents to train on, but perhaps not too high to avoid deviating too much from the desired target metric **NDCG@k**

-  ``lambdarank_norm`` :raw-html:`<a id="lambdarank_norm" title="Permalink to this parameter" href="#lambdarank_norm">&#x1F517;&#xFE0E;</a>`, 기본값 = ``true``, 타입 = bool

   -  used only in ``lambdarank`` application

   -  set this to ``true`` to normalize the lambdas for different queries, and improve the performance for unbalanced data

   -  set this to ``false`` to enforce the original lambdarank algorithm

-  ``label_gain`` :raw-html:`<a id="label_gain" title="Permalink to this parameter" href="#label_gain">&#x1F517;&#xFE0E;</a>`, 기본값 = ``0,1,3,7,15,31,63,...,2^30-1``, 타입 = multi-double

   -  used only in ``lambdarank`` application

   -  relevant gain for labels. For example, the gain of label ``2`` is ``3`` in case of default label gains

   -  separate by ``,``

Metric Parameters
-----------------

-  ``metric`` :raw-html:`<a id="metric" title="Permalink to this parameter" href="#metric">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = multi-enum, 별칭: ``metrics``, ``metric_types``

   -  metric(s) to be evaluated on the evaluation set(s)

      -  ``""`` (empty string or not specified) means that metric corresponding to specified ``objective`` will be used (this is possible only for pre-defined objective functions, otherwise no evaluation metric will be added)

      -  ``"None"`` (string, **not** a ``None`` value) means that no metric will be registered, 별칭: ``na``, ``null``, ``custom``

      -  ``l1``, absolute loss, 별칭: ``mean_absolute_error``, ``mae``, ``regression_l1``

      -  ``l2``, square loss, 별칭: ``mean_squared_error``, ``mse``, ``regression_l2``, ``regression``

      -  ``rmse``, root square loss, 별칭: ``root_mean_squared_error``, ``l2_root``

      -  ``quantile``, `Quantile regression <https://en.wikipedia.org/wiki/Quantile_regression>`__

      -  ``mape``, `MAPE loss <https://en.wikipedia.org/wiki/Mean_absolute_percentage_error>`__, 별칭: ``mean_absolute_percentage_error``

      -  ``huber``, `Huber loss <https://en.wikipedia.org/wiki/Huber_loss>`__

      -  ``fair``, `Fair loss <https://www.kaggle.com/c/allstate-claims-severity/discussion/24520>`__

      -  ``poisson``, negative log-likelihood for `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__

      -  ``gamma``, negative log-likelihood for **Gamma** regression

      -  ``gamma_deviance``, residual deviance for **Gamma** regression

      -  ``tweedie``, negative log-likelihood for **Tweedie** regression

      -  ``ndcg``, `NDCG <https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG>`__, 별칭: ``lambdarank``, ``rank_xendcg``, ``xendcg``, ``xe_ndcg``, ``xe_ndcg_mart``, ``xendcg_mart``

      -  ``map``, `MAP <https://makarandtapaswi.wordpress.com/2012/07/02/intuition-behind-average-precision-and-map/>`__, 별칭: ``mean_average_precision``

      -  ``auc``, `AUC <https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve>`__

      -  ``average_precision``, `average precision score <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html>`__

      -  ``binary_logloss``, `log loss <https://en.wikipedia.org/wiki/Cross_entropy>`__, 별칭: ``binary``

      -  ``binary_error``, for one sample: ``0`` for correct classification, ``1`` for error classification

      -  ``auc_mu``, `AUC-mu <http://proceedings.mlr.press/v97/kleiman19a/kleiman19a.pdf>`__

      -  ``multi_logloss``, log loss for multi-class classification, 별칭: ``multiclass``, ``softmax``, ``multiclassova``, ``multiclass_ova``, ``ova``, ``ovr``

      -  ``multi_error``, error rate for multi-class classification

      -  ``cross_entropy``, cross-entropy (with optional linear weights), 별칭: ``xentropy``

      -  ``cross_entropy_lambda``, "intensity-weighted" cross-entropy, 별칭: ``xentlambda``

      -  ``kullback_leibler``, `Kullback-Leibler divergence <https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>`__, 별칭: ``kldiv``

   -  support multiple metrics, separated by ``,``

-  ``metric_freq`` :raw-html:`<a id="metric_freq" title="Permalink to this parameter" href="#metric_freq">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1``, 타입 = 정수형, 별칭: ``output_freq``, 제약 조건: ``metric_freq > 0``

   -  frequency for metric output

   -  **참고**: CLI 버전에서만 사용 가능합니다.

-  ``is_provide_training_metric`` :raw-html:`<a id="is_provide_training_metric" title="Permalink to this parameter" href="#is_provide_training_metric">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool, 별칭: ``training_metric``, ``is_training_metric``, ``train_metric``

   -  set this to ``true`` to output metric result over training dataset

   -  **참고**: CLI 버전에서만 사용 가능합니다.

-  ``eval_at`` :raw-html:`<a id="eval_at" title="Permalink to this parameter" href="#eval_at">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1,2,3,4,5``, 타입 = multi-int, 별칭: ``ndcg_eval_at``, ``ndcg_at``, ``map_eval_at``, ``map_at``

   -  used only with ``ndcg`` and ``map`` metrics

   -  `NDCG <https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG>`__ and `MAP <https://makarandtapaswi.wordpress.com/2012/07/02/intuition-behind-average-precision-and-map/>`__ evaluation positions, separated by ``,``

-  ``multi_error_top_k`` :raw-html:`<a id="multi_error_top_k" title="Permalink to this parameter" href="#multi_error_top_k">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1``, 타입 = 정수형, 제약 조건: ``multi_error_top_k > 0``

   -  used only with ``multi_error`` metric

   -  threshold for top-k multi-error metric

   -  the error on each sample is ``0`` if the true class is among the top ``multi_error_top_k`` predictions, and ``1`` otherwise

      -  more precisely, the error on a sample is ``0`` if there are at least ``num_classes - multi_error_top_k`` predictions strictly less than the prediction on the true class

   -  when ``multi_error_top_k=1`` this is equivalent to the usual multi-error metric

-  ``auc_mu_weights`` :raw-html:`<a id="auc_mu_weights" title="Permalink to this parameter" href="#auc_mu_weights">&#x1F517;&#xFE0E;</a>`, 기본값 = ``None``, 타입 = multi-double

   -  used only with ``auc_mu`` metric

   -  list representing flattened matrix (in row-major order) giving loss weights for classification errors

   -  list should have ``n * n`` elements, where ``n`` is the number of classes

   -  the matrix co-ordinate ``[i, j]`` should correspond to the ``i * n + j``-th element of the list

   -  if not specified, will use equal weights for all classes

Network Parameters
------------------

-  ``num_machines`` :raw-html:`<a id="num_machines" title="Permalink to this parameter" href="#num_machines">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1``, 타입 = 정수형, 별칭: ``num_machine``, 제약 조건: ``num_machines > 0``

   -  the number of machines for distributed learning application

   -  this parameter is needed to be set in both **socket** and **mpi** versions

-  ``local_listen_port`` :raw-html:`<a id="local_listen_port" title="Permalink to this parameter" href="#local_listen_port">&#x1F517;&#xFE0E;</a>`, 기본값 = ``12400 (random for Dask-package)``, 타입 = 정수형, 별칭: ``local_port``, ``port``, 제약 조건: ``local_listen_port > 0``

   -  TCP listen port for local machines

   -  **참고**: don't forget to allow this port in firewall settings before training

-  ``time_out`` :raw-html:`<a id="time_out" title="Permalink to this parameter" href="#time_out">&#x1F517;&#xFE0E;</a>`, 기본값 = ``120``, 타입 = 정수형, 제약 조건: ``time_out > 0``

   -  socket time-out in minutes

-  ``machine_list_filename`` :raw-html:`<a id="machine_list_filename" title="Permalink to this parameter" href="#machine_list_filename">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열, 별칭: ``machine_list_file``, ``machine_list``, ``mlist``

   -  path of file that lists machines for this distributed learning application

   -  each line contains one IP and one port for one machine. The format is ``ip port`` (space as a separator)

   -  **참고**: CLI 버전에서만 사용 가능합니다.

-  ``machines`` :raw-html:`<a id="machines" title="Permalink to this parameter" href="#machines">&#x1F517;&#xFE0E;</a>`, 기본값 = ``""``, 타입 = 문자열, 별칭: ``workers``, ``nodes``

   -  list of machines in the following format: ``ip1:port1,ip2:port2``

GPU Parameters
--------------

-  ``gpu_platform_id`` :raw-html:`<a id="gpu_platform_id" title="Permalink to this parameter" href="#gpu_platform_id">&#x1F517;&#xFE0E;</a>`, 기본값 = ``-1``, 타입 = 정수형

   -  OpenCL platform ID. Usually each GPU vendor exposes one OpenCL platform

   -  ``-1`` means the system-wide default platform

   -  **참고**: refer to `GPU Targets <./GPU-Targets.rst#query-opencl-devices-in-your-system>`__ for more details

-  ``gpu_device_id`` :raw-html:`<a id="gpu_device_id" title="Permalink to this parameter" href="#gpu_device_id">&#x1F517;&#xFE0E;</a>`, 기본값 = ``-1``, 타입 = 정수형

   -  OpenCL device ID in the specified platform. Each GPU in the selected platform has a unique device ID

   -  ``-1`` means the default device in the selected platform

   -  **참고**: refer to `GPU Targets <./GPU-Targets.rst#query-opencl-devices-in-your-system>`__ for more details

-  ``gpu_use_dp`` :raw-html:`<a id="gpu_use_dp" title="Permalink to this parameter" href="#gpu_use_dp">&#x1F517;&#xFE0E;</a>`, 기본값 = ``false``, 타입 = bool

   -  set this to ``true`` to use double precision math on GPU (by default single precision is used)

   -  **참고**: can be used only in OpenCL implementation, in CUDA implementation only double precision is currently supported

-  ``num_gpu`` :raw-html:`<a id="num_gpu" title="Permalink to this parameter" href="#num_gpu">&#x1F517;&#xFE0E;</a>`, 기본값 = ``1``, 타입 = 정수형, 제약 조건: ``num_gpu > 0``

   -  number of GPUs

   -  **참고**: can be used only in CUDA implementation

.. end params list

Others
------

Continued Training with Input Score
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

LightGBM supports continued training with initial scores. It uses an additional file to store these initial scores, like the following:

::

    0.5
    -0.1
    0.9
    ...

It means the initial score of the first data row is ``0.5``, second is ``-0.1``, and so on.
The initial score file corresponds with data file line by line, and has per score per line.

And if the name of data file is ``train.txt``, the initial score file should be named as ``train.txt.init`` and placed in the same folder as the data file.
In this case, LightGBM will auto load initial score file if it exists.

Weight Data
~~~~~~~~~~~

LightGBM supports weighted training. It uses an additional file to store weight data, like the following:

::

    1.0
    0.5
    0.8
    ...

It means the weight of the first data row is ``1.0``, second is ``0.5``, and so on.
The weight file corresponds with data file line by line, and has per weight per line.

And if the name of data file is ``train.txt``, the weight file should be named as ``train.txt.weight`` and placed in the same folder as the data file.
In this case, LightGBM will load the weight file automatically if it exists.

Also, you can include weight column in your data file. Please refer to the ``weight_column`` `parameter <#weight_column>`__ in above.

Query Data
~~~~~~~~~~

For learning to rank, it needs query information for training data.

LightGBM uses an additional file to store query data, like the following:

::

    27
    18
    67
    ...

For wrapper libraries like in Python and R, this information can also be provided as an array-like via the Dataset parameter ``group``.

::

    [27, 18, 67, ...]

For example, if you have a 112-document dataset with ``group = [27, 18, 67]``, that means that you have 3 groups, where the first 27 records are in the first group, records 28-45 are in the second group, and records 46-112 are in the third group.

**참고**: data should be ordered by the query.

If the name of data file is ``train.txt``, the query file should be named as ``train.txt.query`` and placed in the same folder as the data file.
In this case, LightGBM will load the query file automatically if it exists.

Also, you can include query/group id column in your data file. Please refer to the ``group_column`` `parameter <#group_column>`__ in above.

.. _Laurae++ Interactive Documentation: https://sites.google.com/view/lauraepp/parameters
